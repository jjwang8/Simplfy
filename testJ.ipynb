{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (c:/Users/19259/Documents/Python/Simplify/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n",
      "100%|██████████| 3/3 [00:00<00:00, 62.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.shuffle of Dataset({\n",
       "    features: ['document', 'summary', 'id'],\n",
       "    num_rows: 11334\n",
       "})>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cache = os.path.join(os.path.abspath(os.getcwd()), 'datasets')\n",
    "os.environ['HF_DATASETS_CACHE'] = cache\n",
    "\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "OneSentData = load_dataset(\"xsum\")\n",
    "OneSentData[\"test\"].shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-193b3f554bf14a6b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> document: The first involved a motorcycle and a BMW at about 12:10 BST between Merthyr and Nant Ddu.\n",
      "The second happened 20 minutes later near Treforest, Rhondda Cynon Taff, involving a motorcycle and another vehicle.\n",
      "Diversions are in place in both areas with drivers advised to expect delays. Anyone with information is asked to call South Wales Police on 101.\n",
      ">> summary: Police are investigating two serious crashes on the A470 in south Wales.\n",
      ">> id: 33751856\n",
      "\n",
      "\n",
      ">> document: The Bank of England said the change to the Financial Services Compensation Scheme (FSCS) would happen by 30 January 2017.\n",
      "The compensation limit was lowered to £75,000 in July 2015, following sterling's rise against the euro.\n",
      "But since the Brexit vote, sterling has fallen more than 10% against the euro.\n",
      "The amount of compensation payable is set at €100,000 across the European Union, and under normal circumstances is re-calculated in sterling every five years.\n",
      "But the European directive involved allows a more frequent assessment where there are \"unforeseen events, such as currency fluctuations\".\n",
      "The Bank of England's move suggests that it believes the fall in the value of sterling is at least semi-permanent.\n",
      "The FSCS covers current and savings accounts, and cash ISAs.\n",
      "The new maximum compensation - of £85,000 per person per bank - was previously in place for five years, up to July 2015.\n",
      "However Andrew Tyrie, the chairman of the Treasury Select Committee, said the level had been altered seven times in the last ten years.\n",
      "\"The absurd situation, in which the UK is left vulnerable, at the discretion of the European Commission, to frequent changes in our deposit scheme, must be brought to an end,\" he said.\n",
      "\"Brexit should give the UK the opportunity to set its own level of protection. We should take it.\"\n",
      "The Prudential Regulation Authority (PRA), part of the Bank of England, will now consult on the plan, although the idea has already been approved by the Treasury and the European Commission.\n",
      "The consultation will close in less than a month, on 16 December.\n",
      "Banks and building societies will have until next summer to change the conditions on their publicity material.\n",
      ">> summary: The maximum compensation payable to account holders and savers if their bank collapses is to return to its previous limit of £85,000.\n",
      ">> id: 38050155\n",
      "\n",
      "\n",
      ">> document: The chief constable of the force has been summoned to appear before the Home Affairs Select Committee of MPs following a BBC Wales investigation.\n",
      "Committee chairman Keith Vaz has called for an urgent investigation.\n",
      "BTP has identified improvements.\n",
      "One of the whistleblowers who spoke to BBC Wales was a retired detective chief inspector with 30 years experience.\n",
      "Jon Williams took on a job to review information BTP has on people relating to issues like criminal records and the likelihood of violence or sexual offences in April 2013.\n",
      "He said he and two colleagues started raising concerns within weeks of starting the job.\n",
      "QUALITY CONCERNS\n",
      "Mr Williams claims the quality and management of data at BTP undermined the Police National Database - which was created after the 2002 Soham murders, to ensure police forces shared intelligence.\n",
      "It brings together 150 different computer systems and combines information from the 43 police forces in England and Wales\n",
      "The system is based on a nominal record being created for any individual the police has intelligence on. It holds basic details like a name, date of birth and any aliases.\n",
      "Intelligence is then attached to the record so when a search on an individual is made, all the information is available.\n",
      "Mr Williams said: \"BTP wasn't creating nominal records so [only] the individual reports were being uploaded. But we couldn't find them.\n",
      "\"If another force searched on name details of an individual they couldn't find it. We couldn't find it. So that meant nobody else could find it.\"\n",
      "He also claimed people who had committed minor crimes were mixed up with sex offenders and violent criminals.\n",
      "Mr Williams said the potential consequences were that intelligence on an individual which would be the key piece in the jigsaw for solving or preventing a serious crime could be missing.\n",
      "CONCERNS OVER POSSIBLE CORRUPTION\n",
      "Mr Williams also reported allegations of possible organisational corruption but claims they were not investigated for months.\n",
      "He said that an employee from the crime recording department told him that he and his colleagues had been told to record crimes in order to increase funding from the train operating companies that pay for BTP.\n",
      "\"Now, I don't know whether there is corruption in that process. What I find strange, in my experience of 30 years policing, is that nobody thought to investigate what we said for the best part of eight or nine months.\"\n",
      "\"If somebody is fiddling the crime figures, that's very serious so you would have thought they would have investigated it, but it wasn't investigated. If it was, I was never spoken to and nobody has ever interviewed me as a witness to what was said.\"\n",
      "DATA CONCERNS\n",
      "Mr Williams also reported breaches of the Data Protection Act.\n",
      "According to police policy, information kept on people should be disposed of if it has no policing purpose.\n",
      "But Mr Williams said that he knows of at least 11,000 intelligence reports held on the force intelligence system that were earmarked for deletion but had not been removed.\n",
      "British Transport Police also has around 10,000 boxes of personal information kept in archive storage, dating back before 2006.\n",
      "QUESTIONS FROM MPS\n",
      "Keith Vaz wants an urgent independent investigation and has called on the BTP chief constable to give evidence to the committee about why the information was so unclear \"and why action has not been taken to make the database fit for purpose\".\n",
      "He added: \"This undermines the whole basis of the criminal justice system.\"\n",
      "BRITISH TRANSPORT POLICE RESPONSE:\n",
      "BTP said it recognises its obligations to record information accurately and welcomes the opportunity for its chief constable to appear before MPs.\n",
      "\"In January 2014, we carried out a review and identified ways to improve how our records were more searchable on the Police National Database.\n",
      "\"As a result of this review, measures were put in place to ensure our records were appropriately linked.\"\n",
      "THE WHISTLEBLOWERS - WHAT HAPPENED NEXT?\n",
      "Jon Williams said he was frustrated at the lack of action so on 9 July 2014, the three whistleblowers met with Simon Downey, the head of capability and resources to repeat their concerns.\n",
      "The following day they were told they were being investigated for bullying and they were suspended in August.\n",
      "\"We weren't told why we were suspended. About two weeks after the suspension, I had a letter which purported to explain the reasons why. It didn't.\"\n",
      "He said they feel \"abandoned\" by the force and believes complaining to BTP about bad data management processes was \"a significant factor in the allegations made against us.\"\n",
      "The three resigned in October and are now taking a case for constructive dismissal against BTP.\n",
      "The force said in a statement it intended to \"resist all allegations that staff were treated unfairly after raising concerns about the management of data and information\".\n",
      ">> summary: The safety of the public and police has been put at risk because of the way British Transport Police handles information at its offices in Cardiff, three whistleblowers claim.\n",
      ">> id: 30847519\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def samples(dataset, samples=3, seed=0):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(samples))\n",
    "    for i in sample:\n",
    "        for l in dataset[\"train\"].column_names:\n",
    "            print(f\">> {l}: {i[l]}\")\n",
    "        print(\"\\n\")\n",
    "samples(OneSentData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-aa02ff01c7986985.arrow\n",
      "Loading cached processed dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-02dabe45a731619a.arrow\n",
      "Loading cached processed dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-ac6761acabe119fb.arrow\n",
      "Loading cached shuffled indices for dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-193b3f554bf14a6b.arrow\n",
      "Loading cached shuffled indices for dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-2814765f9cebb7ef.arrow\n",
      "Loading cached shuffled indices for dataset at c:\\Users\\19259\\Documents\\Python\\Simplify\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71\\cache-6adcfdde3bd59966.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 40809\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 2266\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 2266\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_size(example):\n",
    "    return (\n",
    "        len(example[\"summary\"]) >= 10\n",
    "    )\n",
    "OneSentData.filter(filter_size)\n",
    "\n",
    "OneSentData[\"train\"] = OneSentData[\"train\"].shuffle(seed=0).select(range(len(OneSentData[\"train\"])//10))\n",
    "OneSentData[\"validation\"] = OneSentData[\"validation\"].shuffle(seed=0).select(range(len(OneSentData[\"validation\"])//10))\n",
    "OneSentData[\"test\"] = OneSentData[\"test\"].shuffle(seed=0).select(range(len(OneSentData[\"test\"])//10))\n",
    "OneSentData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = os.path.join(os.path.abspath(os.getcwd()), 'models')\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, cache_dir= cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 100, 2638, 2600, 5, 27689, 3100, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"I loved reading the Hunger Games!\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'I', 'Ġloved', 'Ġreading', 'Ġthe', 'ĠHunger', 'ĠGames', '!', '</s>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 256\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"document\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tOneSent = OneSentData.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"document\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"summary\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\19259\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.1860323869039647,\n",
       " 'rouge2': 0.026147468479358518,\n",
       " 'rougeL': 0.12072295495837032,\n",
       " 'rougeLsum': 0.14590201775179126}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "score = evaluate_baseline(OneSentData[\"validation\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 4\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tOneSent[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"models/{model_name}-finetuned-test1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=0,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tOneSent.remove_columns(\n",
    "    OneSentData[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   133,    78,   963,    10, 10218,     8,    10,  8588,    23,\n",
       "            59,   316,    35,   698, 28964,   227,  4213,   212,  4503,     8,\n",
       "           234,   927,   211,  6588,     4, 50118,   133,   200,  1102,   291,\n",
       "           728,   423,   583,  6213, 28236,     6,  8778,  2832,  6106, 43042,\n",
       "           261,   255,  3707,     6,  3329,    10, 10218,     8,   277,  1155,\n",
       "             4, 50118,   495, 10744,  2485,    32,    11,   317,    11,   258,\n",
       "           911,    19,  2377,  5578,     7,  1057,  6091,     4,  6142,    19,\n",
       "           335,    16,   553,     7,   486,   391,  5295,   522,    15,  6560,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1],\n",
       "        [    0,   133,   788,     9,  1156,    26,     5,   464,     7,     5,\n",
       "          2108,  1820, 35018, 21759,    36,   597,  3632,   104,    43,    74,\n",
       "          1369,    30,   389,   644,   193,     4, 50118,   133,  4660,  3000,\n",
       "            21,  8206,     7,   984,  2545,     6,   151,    11,   550,   570,\n",
       "             6,   511, 17179,    18,  1430,   136,     5,  2287,     4, 50118,\n",
       "          1708,   187,     5,  2404,   900,     6, 17179,    34,  4491,    55,\n",
       "            87,   158,   207,   136,     5,  2287,     4, 50118,   133,  1280,\n",
       "             9,  4660, 21467,    16,   278,    23,  4480,  1866,     6,   151,\n",
       "           420,     5,   796,  1332,     6,     8,   223,  2340,  4215,    16,\n",
       "           769,    12, 11762, 41244,    11, 17179,   358,   292,   107,     4,\n",
       "         50118,  1708,     5,   796, 16924,   963,  2386,    10,    55,  7690,\n",
       "          4990,   147,    89,    32,    22,   879, 47765,  1061,     6,   215,\n",
       "            25,  2593, 22798,   845, 50118,   133,   788,     9,  1156,    18,\n",
       "           517,  3649,    14,    24,  2046,     5,  1136,    11,     5,   923,\n",
       "             9, 17179,    16,    23,   513,  4126,    12,  1741, 39713,     4,\n",
       "         50118,   133,   274,  3632,   104,  4865,   595,     8,  4522,  2349,\n",
       "             6,     8,  1055,  3703,  1620,     4, 50118,   133,    92,  4532,\n",
       "          4660,   111,     9,   984,  4531,     6,   151,   228,   621,   228,\n",
       "           827,   111,    21,  1433,    11,   317,    13,   292,   107,     6,\n",
       "            62,     7,   550,   570,     4, 50118, 10462,  2224,  5957,  3636,\n",
       "             6,     5,  2243,     9,     5,  4732, 10908,  1674,     6,    26,\n",
       "             5,   672,    56,    57, 15992,   707,   498,    11,     5,    94,\n",
       "          2724,   107,     4, 50118,   113,   133, 16091,  1068,     6,    11,\n",
       "            61,     5,   987,    16,   314,  4478,     6,    23,     5, 14145,\n",
       "             9,     5,   796,  1463,     6,     7,  7690,  1022,    11,    84,\n",
       "          8068,  3552,     6,   531,    28,  1146,     7,    41,   253,    60,\n",
       "            37,    26,     4, 50118,   113, 16636,   197,   492,     5,   987,\n",
       "             5,   945,     7,   278,    63,   308,   672,     9,  2591,     4,\n",
       "           166,   197,   185,    24,    72, 50118,   133,  2869,  1906, 12986,\n",
       "         18912,  4305,    36,   510,  4396,   238,   233,     9,     5,   788,\n",
       "             9,  1156,     6,    40,   122, 12777,    15,     5,   563,     6,\n",
       "          1712,     5,  1114,    34,   416,    57,  2033,    30,     5,  4732,\n",
       "             8,     5,   796,  1463,     4, 50118,   133,  9434,    40,   593,\n",
       "            11,   540,    87,    10,   353,     6,    15,   545,   719,     4,\n",
       "         50118,   387, 10950,     8,   745, 17537,    40,    33,   454,   220,\n",
       "          1035,     7,   464,     5,  1274,    15,    49, 13698,  1468,     4,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]]), 'labels': tensor([[    0,  9497,    32,  3219,    80,  1473, 12328,    15,     5,    83,\n",
       "         29748,    11,  2077,  5295,     4,     2,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    0,   133,  4532,  4660, 21467,     7,  1316,  9758,     8,  2241,\n",
       "          3697,   114,    49,   827, 33947,    16,     7,   671,     7,    63,\n",
       "           986,  3000,     9,   984,  4531,     6,   151,     4,     2]]), 'decoder_input_ids': tensor([[    2,     0,  9497,    32,  3219,    80,  1473, 12328,    15,     5,\n",
       "            83, 29748,    11,  2077,  5295,     4,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    2,     0,   133,  4532,  4660, 21467,     7,  1316,  9758,     8,\n",
       "          2241,  3697,   114,    49,   827, 33947,    16,     7,   671,     7,\n",
       "            63,   986,  3000,     9,   984,  4531,     6,   151,     4]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19259\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  1%|          | 402/40816 [02:32<4:00:01,  2.81it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\19259\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1665\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1666\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1667\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1668\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1669\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\19259\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1940\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1938\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1939\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1940\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1942\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1943\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1944\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1945\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1946\u001b[0m ):\n\u001b[0;32m   1947\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\19259\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2753\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2751\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   2752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2753\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2755\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\19259\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\19259\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "trained_model = \"models\\\\bart-base-finetuned-test1\\checkpoint-10000\"\n",
    "summarizer = pipeline(\"summarization\", model=trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The maximum compensation payable to banks and building societies for a fall in the value of sterling is to be reduced.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\"\"\"The Bank of England said the change to the Financial Services Compensation Scheme (FSCS) would happen by 30 January 2017.\n",
    "The compensation limit was lowered to £75,000 in July 2015, following sterling's rise against the euro.\n",
    "But since the Brexit vote, sterling has fallen more than 10% against the euro.\n",
    "The amount of compensation payable is set at €100,000 across the European Union, and under normal circumstances is re-calculated in sterling every five years.\n",
    "But the European directive involved allows a more frequent assessment where there are \"unforeseen events, such as currency fluctuations\".\n",
    "The Bank of England's move suggests that it believes the fall in the value of sterling is at least semi-permanent.\n",
    "The FSCS covers current and savings accounts, and cash ISAs.\n",
    "The new maximum compensation - of £85,000 per person per bank - was previously in place for five years, up to July 2015.\n",
    "However Andrew Tyrie, the chairman of the Treasury Select Committee, said the level had been altered seven times in the last ten years.\n",
    "\"The absurd situation, in which the UK is left vulnerable, at the discretion of the European Commission, to frequent changes in our deposit scheme, must be brought to an end,\" he said.\n",
    "\"Brexit should give the UK the opportunity to set its own level of protection. We should take it.\"\n",
    "The Prudential Regulation Authority (PRA), part of the Bank of England, will now consult on the plan, although the idea has already been approved by the Treasury and the European Commission.\n",
    "The consultation will close in less than a month, on 16 December.\n",
    "Banks and building societies will have until next summer to change the conditions on their publicity material.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
